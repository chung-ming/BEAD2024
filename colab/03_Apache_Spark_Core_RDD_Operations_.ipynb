{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CWLfwBSfIFai4_aEkQ7qYT8H5ENZIN0O",
      "authorship_tag": "ABX9TyPN5Ns80CoqczWeyFE6RBnv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2024/blob/main/colab/03_Apache_Spark_Core_RDD_Operations_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Apache PySpark\n",
        "In this demo we will see how we can run PySpark in a Google\n",
        " Colaboratory notebook. We will also perform some basic data exploratory tasks common to data science problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "jaVSpe-PbxkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark Install\n",
        "\n",
        "The first step involves installing pyspark.  The next step is to install findspark library.\n",
        "\n",
        "*Note: the --ignore-install flag is used to ignore previous installations and use the latest one built alongside the allocated cluster.*\n"
      ],
      "metadata": {
        "id": "26ugKWTTdmm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install pyspark using pip\n",
        "!pip install --ignore-install -q pyspark\n",
        "# install findspark using pip\n",
        "!pip install --ignore-install -q findspark"
      ],
      "metadata": {
        "id": "7cdc0oJjb8N5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b275d02a-6720-42db-8718-b5421d89857b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Session\n",
        "\n",
        "We import the basic object SparkSession from the Spark Framework. In PySpark, a Spark Session is a unified entry point for reading data, configuring the system, and managing various Spark services.\n",
        "\n",
        "Here's a breakdown of what the Spark Session does:\n",
        "\n",
        "1. Unified Entry Point: It's the central point to access all Spark  functionalities, making it simpler and more intuitive to use Spark for development.\n",
        "2. Data Reading and Writing: We use the Spark Session to read data from various sources (like HDFS, S3, JDBC, Hive, etc.) and write data to various sinks.\n",
        "3. Configuration Management: It allows us to configure various aspects of the Spark application, such as setting configuration parameters.\n",
        "4. Creating DataFrames and Datasets: The Spark Session provides methods to create DataFrames and Datasets, which are the core data structures in Spark.\n",
        "5. Execution of SQL Queries: We can run SQL queries by using the Spark Session, especially when dealing with structured data.\n",
        "6. Managing Spark Services: It also helps in managing underlying Spark services like SparkContext, and it's the main point of interaction when dealing with structured data.\n",
        "\n",
        "In PySpark, a Spark Session is created using the SparkSession.builder method. Here's an example:"
      ],
      "metadata": {
        "id": "c7coAqaRcsJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# import collections\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"My App \").getOrCreate()"
      ],
      "metadata": {
        "id": "l1I1VynS4JBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext"
      ],
      "metadata": {
        "id": "c-XvTqWhANe5",
        "outputId": "0bf5704d-31ad-4f0d-942f-3dbdaf59fbde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local appName=First Spark and Colab Demo >"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://741fe1287677:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>First Spark and Colab Demo </code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Line Count\n",
        "To count the number of lines from a file."
      ],
      "metadata": {
        "id": "Ux4bAHTnv70Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "someFile = \"somefile.txt\"\n",
        "# the above file is under your pythonProject folder\n",
        "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
        "print(spark.read.text(someFile).count())\n"
      ],
      "metadata": {
        "id": "wRm36sJXv7mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "427b8308-0424-4115-b469-e12b0871c146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Google Drive\n",
        "Connect to Google Drive"
      ],
      "metadata": {
        "id": "AVsBhvHTvokF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddxRp3RNh1Fx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea86fac6-89cb-4a76-c6ab-c068c9997239"
      },
      "source": [
        "# to read in data from a text file, first upload the data file into your google drive and then mount your google drive onto colab\n",
        "from google.colab import drive\n",
        "# to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "someFile = \"/content/drive/MyDrive/customer.csv\"\n",
        "# the above file is under your pythonProject folder\n",
        "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
        "print(spark.read.text(someFile).count())\n"
      ],
      "metadata": {
        "id": "9aPoi-6RvrFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Creation\n",
        "\n",
        "### Create a simple from spark RDD\n",
        "In this examples below we want to see how to create a simple data structure using spark core commands\n",
        "\n",
        "#### Example 1: From RDD\n",
        "To create an RDD using a SparkSession in PySpark, you first need to initialize a SparkSession and then use it to create an RDD. Here's a simple example where we'll create an RDD from a tuple of numbers using a SparkSession"
      ],
      "metadata": {
        "id": "dJWVmhPJdK6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD from a list of numbers\n",
        "numbers = (1, 2, 3, 4, 5)\n",
        "numbers_rdd = spark.sparkContext.parallelize(numbers)\n",
        "# Syntax print(spark.sparkContext.parallelize(\"(A B C)\").collect())\n",
        "print(f\"using collect() function\",numbers_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW2Dw06WllqY",
        "outputId": "14956f32-aa30-4855-9262-b78f91bbfdfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using collect() function [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Alternative is to convert to a new data structure that is print friendly\n",
        "print(f\"using pipeline of functions\")\n",
        "print(tuple(spark.sparkContext.parallelize(numbers).collect()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpBiYXVYFTeR",
        "outputId": "8d18825b-2a82-45a0-93ab-e99e6d13f0c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using pipeline of functions\n",
            "(1, 2, 3, 4, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outfile = spark.read.text(\"out.txt\")"
      ],
      "metadata": {
        "id": "7jZjqGDPF6LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us count the lines in the out.txt"
      ],
      "metadata": {
        "id": "R7FUs4kZGTIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outfile.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZU5TmvCGWk6",
        "outputId": "eff96120-f021-49e3-f5be-bba698a73914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2: From Local File\n",
        "\n",
        "To create an RDD in PySpark by reading data from a CSV file, such as \"customers.csv\", you'll use a SparkSession to read the CSV and then convert the DataFrame to an RDD. Here's a step-by-step example:\n",
        "\n",
        "1. Initialize a SparkSession. (Done)\n",
        "2. Read the \"customers.csv\" file into a DataFrame.\n",
        "3. Convert the DataFrame to an RDD.\n",
        "4. Perform a simple action on the RDD, like counting the number of records.\n",
        "\n",
        "Here's the code snippet for this process:"
      ],
      "metadata": {
        "id": "mguhdx0po7H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers = spark.read.csv(\"customers.csv\", header=True, inferSchema=True).rdd\n",
        "\n",
        "# Perform a simple action: count the number of records\n",
        "record_count = customers.count()\n",
        "print(f\"Number of records: {record_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmFLTKiupPZ-",
        "outputId": "e4a88ff2-2379-4b6b-d639-865de0537fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actions and Transformation\n",
        "\n",
        "In PySpark, operations on RDDs can be broadly classified into two categories: transformations and actions. Transformations create a new RDD from an existing one, while actions return a value after running a computation on the RDD. Below are simple examples demonstrating the use of transformations and actions.\n",
        "\n",
        "###Transformations\n",
        "\n",
        "####Map\n",
        "Applies a function to each element and returns a new RDD.\n"
      ],
      "metadata": {
        "id": "EL3V5wUNridx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize((1, 2, 3, 4, 5))\n",
        "# Traditional Python map(function, collection) (few MBs - GB fails)\n",
        "# Scalabale map (Peta - support)\n",
        "# iterablecollection.map(function) -> Object\n",
        "# collect() Object to collection\n",
        "print(tuple(rdd.map(lambda x: x * x).collect()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3o9yPBOr-ss",
        "outputId": "da64485a-0238-47ba-d96f-2586e88e8eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 4, 9, 16, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Filter\n",
        "Returns a new RDD containing only the elements that satisfy a condition."
      ],
      "metadata": {
        "id": "ytxb4hDSsMkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd.filter(lambda x: x % 2 == 0).collect())  # Keeps even numbers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJKFicO-sSrR",
        "outputId": "ef73dd89-bfb6-4f44-8ea9-e68acf16d0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####FlatMap\n",
        "Similar to map, but each input item can be mapped to 0 or more output items."
      ],
      "metadata": {
        "id": "7hK0FzEUsZQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = spark.sparkContext.parallelize([\"hello world\", \"hi\", \"hello mars\", \"hello jupiter\", \"hello saturn\"])\n",
        "print(words.flatMap(lambda x: x.split(\" \")).collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "0BrAp7Uwskim",
        "outputId": "5d15e8bf-e86b-4917-f3ee-7ea473fbb3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9ba7737d0a7a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hello world\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hello mars\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hello jupiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hello saturn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Distinct\n",
        "Returns a new RDD containing distinct elements from the original RDD."
      ],
      "metadata": {
        "id": "eDWTyCeks1iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(words.flatMap(lambda x: x.split(\" \")).distinct().collect())"
      ],
      "metadata": {
        "id": "74xIHlGchlEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuples = spark.sparkContext.parallelize((1, 1, 2, 3, 3, 4))\n",
        "print(tuples.distinct().collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGk2e8kLs6cr",
        "outputId": "4247aa81-7e76-4fd3-c62b-21ef740bda9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Actions\n",
        "####Collect\n",
        "Returns all the elements of the RDD as an array to the driver program."
      ],
      "metadata": {
        "id": "uO6zm2ZgtPYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tuples.distinct().collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrJG5bFCtaIy",
        "outputId": "007249d5-f22e-479f-d903-28bea457f2e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Count\n",
        "Returns the number of elements in the RDD."
      ],
      "metadata": {
        "id": "w9B3hqYsth7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tuples.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yi1FyZRtlcn",
        "outputId": "615a3bc1-7110-4af2-e2ba-253499c589d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Take\n",
        "Returns an array with the first n elements of the RDD."
      ],
      "metadata": {
        "id": "R6AhdGsltqht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_three = tuples.take(3)\n",
        "print(first_three)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_EERBI9tuQu",
        "outputId": "275354cd-1635-49d3-9003-ea246ec4b963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customers.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q56QYaL7N4OH",
        "outputId": "653b986c-d3c4-43b7-bb9b-a83051d9de38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Index=1, Customer Id='DD37Cf93aecA6Dc', First Name='Sheryl', Last Name='Baxter', Company='Rasmussen Group', City='East Leonard', Country='Chile', Phone 1='229.077.5154', Phone 2='397.884.0519x718', Email='zunigavanessa@smith.info', Subscription Date=datetime.date(2020, 8, 24), Website='http://www.stephenson.com/'),\n",
              " Row(Index=2, Customer Id='1Ef7b82A4CAAD10', First Name='Preston', Last Name='Lozano', Company='Vega-Gentry', City='East Jimmychester', Country='Djibouti', Phone 1='5153435776', Phone 2='686-620-1820x944', Email='vmata@colon.com', Subscription Date=datetime.date(2021, 4, 23), Website='http://www.hobbs.com/')]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Reduce\n",
        "Aggregates the elements of the RDD using a function."
      ],
      "metadata": {
        "id": "Zk6Xizictyfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum = tuples.reduce(lambda a, b: a + b)\n",
        "print(sum)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY7SkGfvt2-g",
        "outputId": "6ce076a7-ed02-4afd-cb3c-f57a1b9a40c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These examples illustrate basic operations in PySpark, allowing you to manipulate and analyze large datasets efficiently. To run these examples, ensure you have a SparkContext (sc) initialized in your PySpark environment.\n",
        "\n",
        "### How to pretty print in PySaprk?\n",
        "\n",
        "The take() function and iteration in PySpark will mimic the pretty print function, but use them wisely."
      ],
      "metadata": {
        "id": "fX1Ur6gyqFDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y1ezBHTOrAf",
        "outputId": "a2ab1811-5840-41a6-ce19-67fd7ccf9da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretty printing has been turned OFF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First five records of customer data set\", customers.take(5))\n",
        "print(\"Not so pretty....\")\n",
        "print(\"Now let us pretty print:\")\n",
        "# To pretty print, you need to iterate\n",
        "for element in customers.take(10):\n",
        "    print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QcNCFRZqkiL",
        "outputId": "fbfda114-1368-4df3-af84-9bd9725b0f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First five records of customer data set [Row(Index=1, Customer Id='DD37Cf93aecA6Dc', First Name='Sheryl', Last Name='Baxter', Company='Rasmussen Group', City='East Leonard', Country='Chile', Phone 1='229.077.5154', Phone 2='397.884.0519x718', Email='zunigavanessa@smith.info', Subscription Date=datetime.date(2020, 8, 24), Website='http://www.stephenson.com/'), Row(Index=2, Customer Id='1Ef7b82A4CAAD10', First Name='Preston', Last Name='Lozano', Company='Vega-Gentry', City='East Jimmychester', Country='Djibouti', Phone 1='5153435776', Phone 2='686-620-1820x944', Email='vmata@colon.com', Subscription Date=datetime.date(2021, 4, 23), Website='http://www.hobbs.com/'), Row(Index=3, Customer Id='6F94879bDAfE5a6', First Name='Roy', Last Name='Berry', Company='Murillo-Perry', City='Isabelborough', Country='Antigua and Barbuda', Phone 1='+1-539-402-0259', Phone 2='(496)978-3969x58947', Email='beckycarr@hogan.com', Subscription Date=datetime.date(2020, 3, 25), Website='http://www.lawrence.com/'), Row(Index=4, Customer Id='5Cef8BFA16c5e3c', First Name='Linda', Last Name='Olsen', Company='Dominguez, Mcmillan and Donovan', City='Bensonview', Country='Dominican Republic', Phone 1='001-808-617-6467x12895', Phone 2='+1-813-324-8756', Email='stanleyblackwell@benson.org', Subscription Date=datetime.date(2020, 6, 2), Website='http://www.good-lyons.com/'), Row(Index=5, Customer Id='053d585Ab6b3159', First Name='Joanna', Last Name='Bender', Company='Martin, Lang and Andrade', City='West Priscilla', Country='Slovakia (Slovak Republic)', Phone 1='001-234-203-0635x76146', Phone 2='001-199-446-3860x3486', Email='colinalvarado@miles.net', Subscription Date=datetime.date(2021, 4, 17), Website='https://goodwin-ingram.com/')]\n",
            "Not so pretty....\n",
            "Now let us pretty print:\n",
            "Row(Index=1, Customer Id='DD37Cf93aecA6Dc', First Name='Sheryl', Last Name='Baxter', Company='Rasmussen Group', City='East Leonard', Country='Chile', Phone 1='229.077.5154', Phone 2='397.884.0519x718', Email='zunigavanessa@smith.info', Subscription Date=datetime.date(2020, 8, 24), Website='http://www.stephenson.com/')\n",
            "Row(Index=2, Customer Id='1Ef7b82A4CAAD10', First Name='Preston', Last Name='Lozano', Company='Vega-Gentry', City='East Jimmychester', Country='Djibouti', Phone 1='5153435776', Phone 2='686-620-1820x944', Email='vmata@colon.com', Subscription Date=datetime.date(2021, 4, 23), Website='http://www.hobbs.com/')\n",
            "Row(Index=3, Customer Id='6F94879bDAfE5a6', First Name='Roy', Last Name='Berry', Company='Murillo-Perry', City='Isabelborough', Country='Antigua and Barbuda', Phone 1='+1-539-402-0259', Phone 2='(496)978-3969x58947', Email='beckycarr@hogan.com', Subscription Date=datetime.date(2020, 3, 25), Website='http://www.lawrence.com/')\n",
            "Row(Index=4, Customer Id='5Cef8BFA16c5e3c', First Name='Linda', Last Name='Olsen', Company='Dominguez, Mcmillan and Donovan', City='Bensonview', Country='Dominican Republic', Phone 1='001-808-617-6467x12895', Phone 2='+1-813-324-8756', Email='stanleyblackwell@benson.org', Subscription Date=datetime.date(2020, 6, 2), Website='http://www.good-lyons.com/')\n",
            "Row(Index=5, Customer Id='053d585Ab6b3159', First Name='Joanna', Last Name='Bender', Company='Martin, Lang and Andrade', City='West Priscilla', Country='Slovakia (Slovak Republic)', Phone 1='001-234-203-0635x76146', Phone 2='001-199-446-3860x3486', Email='colinalvarado@miles.net', Subscription Date=datetime.date(2021, 4, 17), Website='https://goodwin-ingram.com/')\n",
            "Row(Index=6, Customer Id='2d08FB17EE273F4', First Name='Aimee', Last Name='Downs', Company='Steele Group', City='Chavezborough', Country='Bosnia and Herzegovina', Phone 1='(283)437-3886x88321', Phone 2='999-728-1637', Email='louis27@gilbert.com', Subscription Date=datetime.date(2020, 2, 25), Website='http://www.berger.net/')\n",
            "Row(Index=7, Customer Id='EA4d384DfDbBf77', First Name='Darren', Last Name='Peck', Company='Lester, Woodard and Mitchell', City='Lake Ana', Country='Pitcairn Islands', Phone 1='(496)452-6181x3291', Phone 2='+1-247-266-0963x4995', Email='tgates@cantrell.com', Subscription Date=datetime.date(2021, 8, 24), Website='https://www.le.com/')\n",
            "Row(Index=8, Customer Id='0e04AFde9f225dE', First Name='Brett', Last Name='Mullen', Company='Sanford, Davenport and Giles', City='Kimport', Country='Bulgaria', Phone 1='001-583-352-7197x297', Phone 2='001-333-145-0369', Email='asnow@colon.com', Subscription Date=datetime.date(2021, 4, 12), Website='https://hammond-ramsey.com/')\n",
            "Row(Index=9, Customer Id='C2dE4dEEc489ae0', First Name='Sheryl', Last Name='Meyers', Company='Browning-Simon', City='Robersonstad', Country='Cyprus', Phone 1='854-138-4911x5772', Phone 2='+1-448-910-2276x729', Email='mariokhan@ryan-pope.org', Subscription Date=datetime.date(2020, 1, 13), Website='https://www.bullock.net/')\n",
            "Row(Index=10, Customer Id='8C2811a503C7c5a', First Name='Michelle', Last Name='Gallagher', Company='Beck-Hendrix', City='Elaineberg', Country='Timor-Leste', Phone 1='739.218.2516x459', Phone 2='001-054-401-0347x617', Email='mdyer@escobar.net', Subscription Date=datetime.date(2021, 11, 8), Website='https://arias.com/')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Key Operations\n",
        "\n",
        "PySpark examples for key based functions are groupByKey, reduceByKey, and sortByKey operations. Let us look at how they work.\n",
        "\n",
        "####groupByKey\n",
        "This operation groups the values for each key in the RDD into a single sequence.\n",
        "####reduceByKey\n",
        "This operation merges the values for each key using an associative reduce function.\n",
        "####sortByKey\n",
        "This operation sorts the dataset by keys.\n",
        "\n",
        "Let us put together an example to compare and contrast\n"
      ],
      "metadata": {
        "id": "S8HxXONGuoxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([(3, 6),(1, 2),(3, 4)])\n",
        "grouped = rdd.groupByKey()\n",
        "for key, values in grouped.collect():\n",
        "    print(f\"{key}: {tuple(values)}\")\n",
        "reduced = rdd.reduceByKey(lambda a, b: a + b)\n",
        "print(reduced.collect())\n",
        "sorted_rdd = rdd.sortByKey()\n",
        "print(sorted_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf_-cHMhvD0V",
        "outputId": "4587c068-dbd1-4dd1-8e64-ba8a42e01ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3: (6, 4)\n",
            "1: (2,)\n",
            "[(3, 10), (1, 2)]\n",
            "[(1, 2), (3, 6), (3, 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that these operations are transformations and require an action like collect to retrieve the data. Also, keep in mind that groupByKey can cause a lot of data shuffling over the network, and it's generally more efficient to use reduceByKey where possible because it combines output values locally before sending data over the network."
      ],
      "metadata": {
        "id": "SETKMpJDwAZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "\n",
        " The sample() transformation is used to sample a fraction of the data from an RDD. You can sample with or without replacement. Here's how you can use it:\n",
        "\n",
        "####Sampling without replacement\n"
      ],
      "metadata": {
        "id": "3-XSy6Lqwt4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, use the sample function to take a random sample of about 10% of the customers without replacement\n",
        "sampled_customers_rdd = customers.sample(False, 0.1)\n",
        "\n",
        "# Collect the results\n",
        "sampled_customers = sampled_customers_rdd.collect()\n",
        "\n",
        "# Print the sampled list of customers\n",
        "for customer in sampled_customers:\n",
        "    print(customer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rfRqPNOw3Ft",
        "outputId": "ff423083-d641-4d1f-af0b-1824702ad9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(Index=1, Customer Id='DD37Cf93aecA6Dc', First Name='Sheryl', Last Name='Baxter', Company='Rasmussen Group', City='East Leonard', Country='Chile', Phone 1='229.077.5154', Phone 2='397.884.0519x718', Email='zunigavanessa@smith.info', Subscription Date=datetime.date(2020, 8, 24), Website='http://www.stephenson.com/')\n",
            "Row(Index=2, Customer Id='1Ef7b82A4CAAD10', First Name='Preston', Last Name='Lozano', Company='Vega-Gentry', City='East Jimmychester', Country='Djibouti', Phone 1='5153435776', Phone 2='686-620-1820x944', Email='vmata@colon.com', Subscription Date=datetime.date(2021, 4, 23), Website='http://www.hobbs.com/')\n",
            "Row(Index=14, Customer Id='A08A8aF8BE9FaD4', First Name='Kristine', Last Name='Cox', Company='Carpenter-Cook', City='Jodyberg', Country='Sri Lanka', Phone 1='786-284-3358x62152', Phone 2='+1-315-627-1796x8074', Email='holdenmiranda@clarke.com', Subscription Date=datetime.date(2021, 2, 8), Website='https://www.brandt.com/')\n",
            "Row(Index=15, Customer Id='6fEaA1b7cab7B6C', First Name='Faith', Last Name='Lutz', Company='Carter-Hancock', City='Burchbury', Country='Singapore', Phone 1='(781)861-7180x8306', Phone 2='207-185-3665', Email='cassieparrish@blevins-chapman.net', Subscription Date=datetime.date(2022, 1, 26), Website='http://stevenson.org/')\n",
            "Row(Index=22, Customer Id='FBd0Ded4F02a742', First Name='Kiara', Last Name='Houston', Company='Manning, Hester and Arroyo', City='South Alvin', Country='Netherlands', Phone 1='001-274-040-3582x10611', Phone 2='+1-528-175-0973x4684', Email='blanchardbob@wallace-shannon.com', Subscription Date=datetime.date(2020, 9, 15), Website='https://www.reid-potts.com/')\n",
            "Row(Index=23, Customer Id='2FB0FAA1d429421', First Name='Colleen', Last Name='Howard', Company='Greer and Sons', City='Brittanyview', Country='Paraguay', Phone 1='1935085151', Phone 2='(947)115-7711x5488', Email='rsingleton@ryan-cherry.com', Subscription Date=datetime.date(2020, 8, 19), Website='http://paul.biz/')\n",
            "Row(Index=26, Customer Id='09D7D7C8Fe09aea', First Name='Marcus', Last Name='Moody', Company='Giles Ltd', City='Kaitlyntown', Country='Panama', Phone 1='674-677-8623', Phone 2='909-277-5485x566', Email='donnamullins@norris-barrett.org', Subscription Date=datetime.date(2022, 5, 24), Website='https://www.curry.com/')\n",
            "Row(Index=27, Customer Id='aBdfcF2c50b0bfD', First Name='Dakota', Last Name='Poole', Company='Simmons Group', City='Michealshire', Country='Belarus', Phone 1='(371)987-8576x4720', Phone 2='071-152-1376', Email='stacey67@fields.org', Subscription Date=datetime.date(2022, 2, 20), Website='https://sanford-wilcox.biz/')\n",
            "Row(Index=29, Customer Id='3B5dAAFA41AFa22', First Name='Stefanie', Last Name='Fitzpatrick', Company='Santana-Duran', City='Acevedoville', Country='Saint Vincent and the Grenadines', Phone 1='(752)776-3286', Phone 2='+1-472-021-4814x85074', Email='wterrell@clark.com', Subscription Date=datetime.date(2020, 7, 30), Website='https://meyers.com/')\n",
            "Row(Index=35, Customer Id='aA9BAFfBc3710fe', First Name='Faith', Last Name='Moon', Company='Waters, Chase and Aguilar', City='West Marthaburgh', Country='Bahamas', Phone 1='+1-586-217-0359x6317', Phone 2='+1-818-199-1403', Email='willistonya@randolph-baker.com', Subscription Date=datetime.date(2021, 11, 3), Website='https://spencer-charles.info/')\n",
            "Row(Index=50, Customer Id='80F33Fd2AcebF05', First Name='Latoya', Last Name='Mccann', Company='Hobbs, Garrett and Sanford', City='Port Sergiofort', Country='Belarus', Phone 1='(530)287-4548x29481', Phone 2='162-234-0249x32790', Email='bobhammond@barry.biz', Subscription Date=datetime.date(2021, 12, 2), Website='https://www.burton.com/')\n",
            "Row(Index=59, Customer Id='8c7DdF10798bCC3', First Name='Kathy', Last Name='Hill', Company='Moore, Mccoy and Glass', City='Selenabury', Country='South Georgia and the South Sandwich Islands', Phone 1='001-171-716-2175x310', Phone 2='888.625.0654', Email='ncamacho@boone-simmons.org', Subscription Date=datetime.date(2020, 11, 15), Website='http://hayden.com/')\n",
            "Row(Index=64, Customer Id='FCBdfCEAe20A8Dc', First Name='Chloe', Last Name='Hutchinson', Company='Simon LLC', City='South Julia', Country='Netherlands', Phone 1='981-544-9452', Phone 2='+1-288-552-4666x060', Email='leah85@sutton-terrell.com', Subscription Date=datetime.date(2022, 5, 15), Website='https://mitchell.info/')\n",
            "Row(Index=95, Customer Id='BE91A0bdcA49Bbc', First Name='Darrell', Last Name='Douglas', Company='Newton, Petersen and Mathis', City='Daisyborough', Country='Mali', Phone 1='001-084-845-9524x1777', Phone 2='001-769-564-6303', Email='grayjean@lowery-good.com', Subscription Date=datetime.date(2022, 2, 17), Website='https://banks.biz/')\n",
            "Row(Index=100, Customer Id='2354a0E336A91A1', First Name='Clarence', Last Name='Haynes', Company='Le, Nash and Cross', City='Judymouth', Country='Honduras', Phone 1='(753)813-6941', Phone 2='783.639.1472', Email='colleen91@faulkner.biz', Subscription Date=datetime.date(2020, 3, 11), Website='http://www.hatfield-saunders.net/')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the sample method:\n",
        "\n",
        "1. The first argument is withReplacement. Set it to False for sampling without replacement, meaning a particular customer can be chosen only once.\n",
        "2. The second argument is the fraction of the data to sample, which is 0.1 in this case, meaning approximately 10% of the data.\n",
        "\n",
        "This will output a random sample of the customers from your customers_rdd. The collect() action is used here for demonstration purposes, and it should be used with caution if the dataset is large, as it will gather all the sampled data to the driver node.\n",
        "\n",
        "####Sampling with replacement\n",
        "\n",
        "The following example shows how to use sample() with replacement. This means an element can be included in the sample multiple times."
      ],
      "metadata": {
        "id": "TZYw0oFExAcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, use the sample function to take a random sample of about 10% of the customers with replacement\n",
        "sampled_customers_rdd = customers.sample(True, 0.01)\n",
        "\n",
        "# Collect the results\n",
        "sampled_customers = sampled_customers_rdd.collect()\n",
        "\n",
        "# Print the sampled list of customers\n",
        "for customer in sampled_customers:\n",
        "    print(customer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzQm9EbUxEq4",
        "outputId": "34ab5254-e1e9-4137-ea23-f34542ef6612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(Index=20, Customer Id='0F60FF3DdCd7aB0', First Name='Joanna', Last Name='Kirk', Company='Mays-Mccormick', City='Jamesshire', Country='French Polynesia', Phone 1='(266)131-7001x711', Phone 2='(283)312-5579x11543', Email='tuckerangie@salazar.net', Subscription Date=datetime.date(2021, 9, 24), Website='https://www.camacho.net/')\n",
            "Row(Index=70, Customer Id='CC68FD1D3Bbbf22', First Name='Riley', Last Name='Good', Company='Wade PLC', City='Erikaville', Country='Canada', Phone 1='6977745822', Phone 2='855-436-7641', Email='alex06@galloway.com', Subscription Date=datetime.date(2020, 2, 3), Website='http://conway.org/')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These examples will give you an array of customers sampled from the original RDD. The actual elements in the sample will vary each time you run the code due to the randomness of the sampling process."
      ],
      "metadata": {
        "id": "nJGdC0dfxpg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Transformations\n",
        "In PySpark, you can perform various RDD operations such as union, join, and cartesian (cross) to combine data in different ways. Here are simple examples for each:\n",
        "#### Union\n",
        "The union operation combines two RDDs to form a new RDD that contains elements from both RDDs."
      ],
      "metadata": {
        "id": "QYMoA6iZx7kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two RDDs\n",
        "rdd1 = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2)])\n",
        "rdd2 = spark.sparkContext.parallelize([(\"Charlie\", 3), (\"David\", 4)])\n",
        "\n",
        "# Perform the union operation\n",
        "union_rdd = rdd1.union(rdd2)\n",
        "\n",
        "# Collect and print the results\n",
        "print(union_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr-ClfANyEsJ",
        "outputId": "91063f25-63bd-45d2-bd0f-df5d56705f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alice', 1), ('Bob', 2), ('Charlie', 3), ('David', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Join\n",
        "The join operation combines two RDDs based on their key."
      ],
      "metadata": {
        "id": "4V_PebbQyQ2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create two RDDs with common keys\n",
        "rdd3 = spark.sparkContext.parallelize([(\"Alice\", \"Apple\"), (\"Bob\", \"Banana\")])\n",
        "rdd4 = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2)])\n",
        "\n",
        "# Perform the join operation\n",
        "join_rdd = rdd3.join(rdd4)\n",
        "\n",
        "# Collect and print the results\n",
        "print(join_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsgtMDGHyVK6",
        "outputId": "6d088daf-21c1-41e7-edb8-3a806b3d7c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alice', ('Apple', 1)), ('Bob', ('Banana', 2))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Cross or Catesian\n",
        "The cartesian operation returns all possible pairs of (a, b) where a is in the first RDD and b is in the second RDD."
      ],
      "metadata": {
        "id": "kwcN4mg6ykED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two RDDs\n",
        "rdd5 = spark.sparkContext.parallelize([1, 2])\n",
        "rdd6 = spark.sparkContext.parallelize([\"a\", \"b\"])\n",
        "\n",
        "# Perform the cartesian operation\n",
        "cross_rdd = rdd5.cartesian(rdd6)\n",
        "\n",
        "# Collect and print the results\n",
        "print(cross_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw643DrQypKe",
        "outputId": "a3b37b63-9692-4346-c7d6-c558e102f3bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that the cartesian operation can be very expensive in terms of computation and memory usage, especially with large datasets, because it forms all possible combinations of elements between the two RDDs.\n",
        "\n",
        "### More Actions\n",
        "\n",
        "####save\n",
        "Saving an RDD in PySpark can be done in a variety of formats. Common formats include saving as text files, sequence files, or other file-based data sources. Below are examples of how to save an RDD that contains customer data as a text file.\n",
        "\n",
        "But we will see about this action after the NoSQL lecture.\n",
        "\n",
        "End of Demo\n",
        "\n",
        "Thank you for the patient listening. 🙏🌞"
      ],
      "metadata": {
        "id": "0cMQA5AkywVv"
      }
    }
  ]
}